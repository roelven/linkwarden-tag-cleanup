#!/usr/bin/env python3
"""
Linkwarden Tag Consolidation Script

Applies tag consolidations based on mapping file generated by analyze_tags.py.

Operations:
1. Rename tags (case normalizations)
2. Merge tags (semantic consolidations)
3. Delete unused/low-usage tags

Usage:
    # Dry run (preview changes without applying)
    python3 consolidate_tags.py --api-url https://linkwarden.w22.io/api/v1 \
                                --token YOUR_TOKEN \
                                --mapping consolidation_mapping.json \
                                --dry-run

    # Apply changes
    python3 consolidate_tags.py --api-url https://linkwarden.w22.io/api/v1 \
                                --token YOUR_TOKEN \
                                --mapping consolidation_mapping.json
"""

import argparse
import json
import sys
import time
from typing import Dict, List, Optional
import requests


class TagConsolidator:
    def __init__(self, api_url: str, token: str, dry_run: bool = True):
        self.api_url = api_url.rstrip('/')
        self.token = token
        self.dry_run = dry_run
        self.headers = {
            'Authorization': f'Bearer {token}',
            'Content-Type': 'application/json'
        }
        self.stats = {
            'case_normalizations': 0,
            'semantic_consolidations': 0,
            'deletions': 0,
            'errors': 0,
            'links_updated': 0
        }

    def fetch_tag_by_name(self, tag_name: str) -> Optional[Dict]:
        """Fetch tag details by name."""
        try:
            response = requests.get(
                f"{self.api_url}/tags",
                headers=self.headers,
                timeout=30
            )
            response.raise_for_status()
            data = response.json()

            # Handle both direct array and paginated responses
            tags = data.get('response', data) if isinstance(data, dict) else data

            for tag in tags:
                if tag.get('name') == tag_name:
                    return tag

            return None

        except requests.exceptions.RequestException as e:
            print(f"  ‚úó Error fetching tag '{tag_name}': {e}")
            return None

    def fetch_links_with_tag(self, tag_id: str) -> List[Dict]:
        """Fetch all links that have a specific tag."""
        try:
            # Linkwarden API: GET /links with tag filter
            response = requests.get(
                f"{self.api_url}/links",
                headers=self.headers,
                params={'tagId': tag_id},
                timeout=30
            )
            response.raise_for_status()
            data = response.json()

            # Handle both direct array and paginated responses
            links = data.get('response', data) if isinstance(data, dict) else data

            return links

        except requests.exceptions.RequestException as e:
            print(f"  ‚úó Error fetching links for tag {tag_id}: {e}")
            return []

    def fetch_link_by_id(self, link_id: str) -> Optional[Dict]:
        """Fetch a single link by ID."""
        try:
            response = requests.get(
                f"{self.api_url}/links/{link_id}",
                headers=self.headers,
                timeout=30
            )
            response.raise_for_status()
            data = response.json()
            # Handle wrapped response
            if isinstance(data, dict) and 'response' in data:
                return data['response']
            return data
        except requests.exceptions.RequestException as e:
            print(f"  ‚úó Error fetching link {link_id}: {e}")
            return None

    def update_link_tags(self, link_id: str, new_tags: List[Dict]) -> bool:
        """Update tags for a specific link."""
        if self.dry_run:
            return True

        try:
            # IMPORTANT: Linkwarden API requires the FULL link object
            # We can't just send {"tags": [...]}
            # We must GET the full link, modify tags, then PUT it back

            link = self.fetch_link_by_id(link_id)
            if not link:
                return False

            # Update the tags field
            link['tags'] = new_tags

            # Send the complete link object back
            response = requests.put(
                f"{self.api_url}/links/{link_id}",
                headers=self.headers,
                json=link,
                timeout=30
            )
            response.raise_for_status()
            return True

        except requests.exceptions.RequestException as e:
            error_detail = ""
            try:
                if hasattr(e.response, 'text'):
                    error_detail = f" - {e.response.text[:200]}"
            except:
                pass
            print(f"  ‚úó Error updating link {link_id}: {e}{error_detail}")
            return False

    def rename_tag(self, old_name: str, new_name: str) -> bool:
        """Rename a tag (case normalization)."""
        print(f"\n{'[DRY RUN] ' if self.dry_run else ''}Renaming tag: '{old_name}' ‚Üí '{new_name}'")

        # Fetch the old tag
        old_tag = self.fetch_tag_by_name(old_name)
        if not old_tag:
            print(f"  ‚ö† Tag '{old_name}' not found, skipping")
            return False

        old_tag_id = old_tag['id']
        usage_count = old_tag.get('_count', {}).get('links', 0)
        print(f"  Found tag ID: {old_tag_id} (used in {usage_count} links)")

        # Check if target tag already exists
        new_tag = self.fetch_tag_by_name(new_name)

        if new_tag:
            # Target exists, need to merge
            print(f"  Target tag '{new_name}' already exists, will merge")
            return self.merge_tags(old_name, new_name)

        # Target doesn't exist, simple rename
        if self.dry_run:
            print(f"  ‚úì Would rename tag '{old_name}' to '{new_name}'")
            self.stats['case_normalizations'] += 1
            return True

        try:
            response = requests.put(
                f"{self.api_url}/tags/{old_tag_id}",
                headers=self.headers,
                json={'name': new_name},
                timeout=30
            )
            response.raise_for_status()
            print(f"  ‚úì Renamed tag '{old_name}' to '{new_name}'")
            self.stats['case_normalizations'] += 1
            return True

        except requests.exceptions.RequestException as e:
            print(f"  ‚úó Error renaming tag: {e}")
            self.stats['errors'] += 1
            return False

    def merge_tags(self, source_name: str, target_name: str) -> bool:
        """Merge source tag into target tag."""
        print(f"\n{'[DRY RUN] ' if self.dry_run else ''}Merging tag: '{source_name}' ‚Üí '{target_name}'")

        # Fetch both tags
        source_tag = self.fetch_tag_by_name(source_name)
        target_tag = self.fetch_tag_by_name(target_name)

        if not source_tag:
            print(f"  ‚ö† Source tag '{source_name}' not found, skipping")
            return False

        if not target_tag:
            print(f"  ‚ö† Target tag '{target_name}' not found, will rename instead")
            return self.rename_tag(source_name, target_name)

        source_id = source_tag['id']
        target_id = target_tag['id']

        # Fetch all links with source tag
        links = self.fetch_links_with_tag(source_id)
        print(f"  Found {len(links)} links with tag '{source_name}'")

        if not links:
            print(f"  No links to update, will delete tag")
            return self.delete_tag(source_name)

        # Update each link
        updated_count = 0
        for link in links:
            link_id = link['id']
            current_tags = link.get('tags', [])

            # Remove source tag, add target tag (if not already present)
            new_tags = [t for t in current_tags if t.get('name') != source_name]

            # Check if target tag already on this link
            has_target = any(t.get('name') == target_name for t in new_tags)
            if not has_target:
                # CRITICAL: Must add the FULL target tag object including ID
                # But remove _count field which comes from /tags endpoint
                clean_tag = {k: v for k, v in target_tag.items() if k != '_count'}
                new_tags.append(clean_tag)

            if self.dry_run:
                updated_count += 1
            else:
                if self.update_link_tags(link_id, new_tags):
                    updated_count += 1
                    self.stats['links_updated'] += 1
                else:
                    self.stats['errors'] += 1

                # Rate limiting
                time.sleep(0.1)

        print(f"  ‚úì {'Would update' if self.dry_run else 'Updated'} {updated_count} links")

        # Delete source tag
        if not self.dry_run:
            self.delete_tag(source_name)

        self.stats['semantic_consolidations'] += 1
        return True

    def delete_tag(self, tag_name: str) -> bool:
        """Delete a tag."""
        tag = self.fetch_tag_by_name(tag_name)
        if not tag:
            print(f"  ‚ö† Tag '{tag_name}' not found, skipping deletion")
            return False

        tag_id = tag['id']

        if self.dry_run:
            print(f"  ‚úì Would delete tag '{tag_name}'")
            self.stats['deletions'] += 1
            return True

        try:
            response = requests.delete(
                f"{self.api_url}/tags/{tag_id}",
                headers=self.headers,
                timeout=30
            )
            response.raise_for_status()
            print(f"  ‚úì Deleted tag '{tag_name}'")
            self.stats['deletions'] += 1
            return True

        except requests.exceptions.RequestException as e:
            print(f"  ‚úó Error deleting tag: {e}")
            self.stats['errors'] += 1
            return False

    def apply_consolidation_mapping(self, mapping: Dict):
        """Apply all consolidations from mapping file."""
        print("\n" + "="*80)
        print(f"{'DRY RUN MODE - NO CHANGES WILL BE MADE' if self.dry_run else 'APPLYING TAG CONSOLIDATIONS'}")
        print("="*80)

        # Phase 1: Case normalizations
        print(f"\nüìù Phase 1: Case Normalizations ({len(mapping['case_normalizations'])} tags)")
        print("-"*80)

        for old_name, new_name in mapping['case_normalizations'].items():
            self.rename_tag(old_name, new_name)
            time.sleep(0.2)  # Rate limiting

        # Phase 2: Semantic consolidations
        print(f"\nüìù Phase 2: Semantic Consolidations ({len(mapping['semantic_consolidations'])} tags)")
        print("-"*80)

        for old_name, new_name in mapping['semantic_consolidations'].items():
            self.merge_tags(old_name, new_name)
            time.sleep(0.2)  # Rate limiting

        # Phase 3: Delete low-usage tags
        if mapping.get('tags_to_delete'):
            print(f"\nüìù Phase 3: Deleting Low-Usage Tags ({len(mapping['tags_to_delete'])} tags)")
            print("-"*80)

            for tag_info in mapping['tags_to_delete']:
                tag_name = tag_info['name']
                usage = tag_info['usage']
                print(f"\n{'[DRY RUN] ' if self.dry_run else ''}Deleting tag: '{tag_name}' ({usage} links)")
                self.delete_tag(tag_name)
                time.sleep(0.1)  # Rate limiting

        # Print summary
        self.print_summary()

    def print_summary(self):
        """Print consolidation summary."""
        print("\n" + "="*80)
        print("CONSOLIDATION SUMMARY")
        print("="*80)
        print(f"  Case normalizations:     {self.stats['case_normalizations']:,}")
        print(f"  Semantic consolidations: {self.stats['semantic_consolidations']:,}")
        print(f"  Tags deleted:            {self.stats['deletions']:,}")
        print(f"  Links updated:           {self.stats['links_updated']:,}")
        print(f"  Errors:                  {self.stats['errors']:,}")
        print("="*80)

        if self.dry_run:
            print("\nüí° This was a dry run. No changes were made.")
            print("   Run without --dry-run to apply changes.")
        else:
            print("\n‚úì Consolidation complete!")


def main():
    parser = argparse.ArgumentParser(description='Consolidate Linkwarden tags')
    parser.add_argument('--api-url', required=True, help='Linkwarden API URL')
    parser.add_argument('--token', required=True, help='API authentication token')
    parser.add_argument('--mapping', required=True, help='Consolidation mapping JSON file')
    parser.add_argument('--dry-run', action='store_true',
                       help='Preview changes without applying them')
    parser.add_argument('--skip-case', action='store_true',
                       help='Skip case normalizations')
    parser.add_argument('--skip-semantic', action='store_true',
                       help='Skip semantic consolidations')
    parser.add_argument('--skip-delete', action='store_true',
                       help='Skip tag deletions')

    args = parser.parse_args()

    # Load mapping
    try:
        with open(args.mapping, 'r', encoding='utf-8') as f:
            mapping = json.load(f)
    except Exception as e:
        print(f"‚úó Error loading mapping file: {e}")
        sys.exit(1)

    # Apply filters
    if args.skip_case:
        mapping['case_normalizations'] = {}
    if args.skip_semantic:
        mapping['semantic_consolidations'] = {}
    if args.skip_delete:
        mapping['tags_to_delete'] = []

    # Create consolidator and apply
    consolidator = TagConsolidator(args.api_url, args.token, dry_run=args.dry_run)
    consolidator.apply_consolidation_mapping(mapping)


if __name__ == '__main__':
    main()
